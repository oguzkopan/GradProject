{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7240cc0",
   "metadata": {},
   "source": [
    "# calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e51ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.utils import plot_model \n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import math\n",
    "import pywt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48ef0d",
   "metadata": {},
   "source": [
    "# main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ef018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing averages\n",
    "def yy5(input_data):\n",
    "    A=0\n",
    "    B=0\n",
    "    C=0\n",
    "    D=0\n",
    "    outputavg = []    \n",
    "    for X in input_data:\n",
    "        Y=(X+A+B+C+D)/5\n",
    "        outputavg.append(Y)\n",
    "        D=C\n",
    "        C=B\n",
    "        B=A\n",
    "        A=X\n",
    "\n",
    "    return outputavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2e3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construction of outputs\n",
    "def output(inputdata):\n",
    "    out=[]\n",
    "    for i in range(7, len(inputdata)-1):\n",
    "        out.append(inputdata[i+1])\n",
    "    out = np.append(out, [np.nan])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ed28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#successive values \n",
    "def successive(successive):\n",
    "   \n",
    "    input_data=[]\n",
    "    for i in range(7, len(successive)):\n",
    "       \n",
    "        input_data.append([successive[i-3]]+[successive[i-2]]+[successive[i-1]]+[successive[i]])\n",
    "    return input_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853f164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavelet transform\n",
    "def four_wavelets(training):\n",
    "    input_data=np.array(training)\n",
    "    days = input_data[:,0:4]\n",
    "    \n",
    "    \n",
    "    for row in input_data:\n",
    "            (a, d) = pywt.dwt(days, 'haar')\n",
    "            (a2,d2)=pywt.dwt(a, 'haar') \n",
    "            l3=np.append(a2,d2, axis=1)\n",
    "            l2_3=np.append(l3,d, axis=1)\n",
    "            transformed_df=l2_3\n",
    "    \n",
    "    training=transformed_df\n",
    "    \n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configurations\n",
    "hidden1=32\n",
    "second_layer1=32\n",
    "third_layer1=32\n",
    "forth_layer1=16\n",
    "hidden2=32\n",
    "second_layer2=32\n",
    "third_layer2=32\n",
    "forth_layer2=16\n",
    "hidden3=32\n",
    "second_layer3=32\n",
    "third_layer3=32\n",
    "forth_layer3=16\n",
    "hidden4=32\n",
    "second_layer4=32\n",
    "third_layer4=32\n",
    "forth_layer4=16\n",
    "hidden5=32\n",
    "second_layer5=32\n",
    "third_layer5=32\n",
    "forth_layer5=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72bf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling input files\n",
    "input_data=pd.read_csv('close_is.csv')\n",
    "#input_data=pd.read_csv('open.csv')\n",
    "#input_data=pd.read_csv('close.csv')\n",
    "#input_data=pd.read_csv('min.csv')\n",
    "#input_data=pd.read_csv('max.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e69cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5.2631</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.7036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>20.9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>21.0736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>20.2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>20.8626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>20.4408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3948 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       5.2631\n",
       "0      5.4018\n",
       "1      5.5229\n",
       "2      5.5381\n",
       "3      5.7036\n",
       "4      5.7294\n",
       "...       ...\n",
       "3943  20.9345\n",
       "3944  21.0736\n",
       "3945  20.2748\n",
       "3946  20.8626\n",
       "3947  20.4408\n",
       "\n",
       "[3948 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588bce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construction of input arrays\n",
    "\n",
    "input_data=np.array(input_data)\n",
    "input_data=input_data.reshape(input_data.shape[0])\n",
    "input_data=list(input_data)\n",
    "input_data=np.array(input_data)\n",
    "\n",
    "\n",
    "average=yy5(input_data)\n",
    "input_data_average=successive(average)\n",
    "input_data_successive=successive(input_data)\n",
    "out=output(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "\n",
    "#fig,ax= plt.subplots()\n",
    "#ax.plot(input_data, label='daily_input')\n",
    "#ax.plot(average, label='average_input')\n",
    "#ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00), shadow=True, ncol=2)\n",
    "#plt.title('Input data')\n",
    "#plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b173e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#division of data set into training and test data set\n",
    "N=len(input_data)\n",
    "division_of_training=0.98\n",
    "input_train=input_data_average[:int(N*division_of_training)]\n",
    "input_test=input_data_average[int(N*division_of_training):int(N*1)]\n",
    "\n",
    "successive_train=input_data_successive[:int(N*division_of_training)]\n",
    "successive_test=input_data_successive[int(N*division_of_training):int(N*1)]\n",
    "      \n",
    "second_input_train=successive_train \n",
    "second_input_test=successive_test \n",
    "\n",
    "output_train= out[:int(N*division_of_training)]\n",
    "output_test=out[int(N*division_of_training):int(N*1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50705868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "\n",
    "inputiavg=np.array(input_train)\n",
    "inputiavgt=np.array(input_test)\n",
    "\n",
    "inputsuc=np.array(second_input_train)\n",
    "inputsuct=np.array(second_input_test)\n",
    "\n",
    "subtraction_average_train=inputiavg\n",
    "subtraction_average_test=inputiavgt\n",
    "\n",
    "subtraction_successive_train=inputsuc\n",
    "subtraction_successive_test=inputsuct\n",
    "\n",
    "subtraction_average_train=subtraction_average_train.sum(axis=1)/4\n",
    "subtraction_average_test=subtraction_average_test.sum(axis=1)/4\n",
    "\n",
    "subtraction_successive_train=subtraction_successive_train.sum(axis=1)/4\n",
    "subtraction_successive_test=subtraction_successive_test.sum(axis=1)/4\n",
    "\n",
    "#normalization of inputs\n",
    "first_input_train=input_train-subtraction_average_train[:, None]\n",
    "first_input_test=input_test-subtraction_average_test[:,None]\n",
    "\n",
    "output_train=output_train-subtraction_successive_train\n",
    "output_test=output_test-subtraction_successive_test\n",
    "\n",
    "second_input_train=second_input_train-subtraction_successive_train[:,None]\n",
    "second_input_test=second_input_test-subtraction_successive_test[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2d5a6",
   "metadata": {},
   "source": [
    "# First NN, primary NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b75fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#4inputs WT\n",
    "final_first_w_input_train=four_wavelets(first_input_train)\n",
    "final_first_w_input_test=four_wavelets(first_input_test)\n",
    "\n",
    "X_train=np.array(final_first_w_input_train[:, 1:])\n",
    "y_train=np.array(output_train)\n",
    "\n",
    "X_test=np.array(final_first_w_input_test[:,1:])\n",
    "y_test=np.array(output_test)\n",
    "\n",
    "m_primary=len(X_train[0,:])\n",
    "p_primary=np.size(y_train[0])\n",
    "N_primary=len(X_train)\n",
    "\n",
    "model= Sequential ([\n",
    "    Dense(hidden1, input_dim=m_primary, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_primary)\n",
    "    ])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "sgd=keras.optimizers.SGD(lr=0.05,momentum=0.75, decay=0.0, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_absolute_error','mean_squared_logarithmic_error','cosine_similarity','logcosh'])\n",
    "history1=model.fit(X_train, y_train, batch_size=N_primary, epochs=300, shuffle=False, verbose=0)  \n",
    "\n",
    "predicted_train = model.predict(X_train) \n",
    "predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "error_train1=predicted_train-y_train\n",
    "\n",
    "predicted_test = model.predict(X_test) \n",
    "predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "error_test1=predicted_test-y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b39a12",
   "metadata": {},
   "source": [
    "# Second NN, error forecasting network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41e6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "error_train=pd.DataFrame(error_train1)\n",
    "add_train=four_wavelets(second_input_train) \n",
    "   \n",
    "X_error_train1=np.array(add_train[:, 1:])\n",
    "y_error_train1=np.array(error_train)\n",
    "\n",
    "error_test=pd.DataFrame(error_test1)\n",
    "add_test=four_wavelets(second_input_test) \n",
    "  \n",
    "X_error_test1=np.array(add_test[:, 1:])\n",
    "\n",
    "m_second=len(X_error_train1[0,:])\n",
    "p_second=np.size(y_train[0])\n",
    "N_second=len(X_error_train1)\n",
    "\n",
    "error_model1= Sequential ([\n",
    "    Dense(hidden2, input_dim=m_second, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_second)\n",
    "])\n",
    "\n",
    "error_model1.summary()\n",
    "\n",
    "sgd=keras.optimizers.SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "error_model1.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "history3=error_model1.fit(X_error_train1, y_error_train1, batch_size=N_second, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "error_predicted_tr = error_model1.predict(X_error_train1)\n",
    "error_predicted_tr = np.reshape(error_predicted_tr, (error_predicted_tr.size,))\n",
    "error_predicted_tes = error_model1.predict(X_error_test1)\n",
    "error_predicted_tes = np.reshape(error_predicted_tes, (error_predicted_tes.size,))\n",
    "\n",
    "compensated1_train=(predicted_train+subtraction_successive_train)-(error_predicted_tr)\n",
    "compensated1_test=(predicted_test+subtraction_successive_test)-(error_predicted_tes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12cff4",
   "metadata": {},
   "source": [
    "# Third NN, error network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314f609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "error_train2a=compensated1_train-(y_train+subtraction_successive_train)\n",
    "error_test2a=compensated1_test-(y_test+subtraction_successive_test)\n",
    "\n",
    "error_train2=pd.DataFrame(error_train2a)\n",
    "error_train2 [1]= error_train2[0].shift(1)\n",
    "error_train2 [2]= error_train2[1].shift(1)\n",
    "error_train2 [3]= error_train2[2].shift(1)\n",
    "error_train2[4]= error_train2[3].shift(1)\n",
    "error_train2 = error_train2.replace(np.nan, 0)\n",
    "\n",
    "##error normalization\n",
    "subtraction_error_train2=np.array(error_train2)\n",
    "subtraction_error_train2=subtraction_error_train2[:,:-1]\n",
    "subtraction_error_train2=subtraction_error_train2.sum(axis=1)/4\n",
    "\n",
    "error_train2=error_train2-subtraction_error_train2[:, None]\n",
    "\n",
    "\n",
    "error_train2=np.array(error_train2)\n",
    "days_train = error_train2[:,1:5]\n",
    "input3_train=four_wavelets(days_train)\n",
    "output3_train=error_train2[:,0:1]\n",
    "\n",
    "X_error_train2=np.array(input3_train[:, 1:])\n",
    "y_error_train2=np.array(output3_train)\n",
    "\n",
    "error_test2=pd.DataFrame(error_test2a)\n",
    "error_test2 [1]= error_test2[0].shift(1)\n",
    "error_test2 [2]=error_test2[1].shift(1)\n",
    "error_test2 [3]=error_test2[2].shift(1)\n",
    "error_test2[4]=error_test2[3].shift(1)\n",
    "error_test2 = error_test2.replace(np.nan, 0)\n",
    "\n",
    "subtraction_error_test2=np.array(error_test2)\n",
    "subtraction_error_test2=subtraction_error_test2[:,:-1]\n",
    "subtraction_error_test2=subtraction_error_test2.sum(axis=1)/4\n",
    "\n",
    "error_test2=error_test2-subtraction_error_test2[:,None]\n",
    "\n",
    "error_test2=np.array(error_test2)\n",
    "days_test = error_test2[:,1:5]\n",
    "input3_test=four_wavelets(days_test)\n",
    "output3_test=error_test2[:,0:1]\n",
    "\n",
    "X_error_test2=np.array(input3_test[:, 1:])\n",
    "\n",
    "\n",
    "#####3rd NN\n",
    "m_error=len(X_error_train2[0,:])\n",
    "p_error=np.size(y_error_train2[0])\n",
    "N_error=len(X_error_train2)\n",
    "\n",
    "\n",
    " \n",
    "error_model2= Sequential ([\n",
    "    Dense(hidden3, input_dim=m_error, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_error)\n",
    "])\n",
    "\n",
    "error_model2.summary()\n",
    "\n",
    "sgd=keras.optimizers.SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "error_model2.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "history4=error_model2.fit(X_error_train2, y_error_train2, batch_size=N_error, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "\n",
    "error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "error_predicted_tes2 = error_model2.predict( X_error_test2)\n",
    "error_predicted_tes2= np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "compensated_y_train=compensated1_train-(error_predicted_tr2+subtraction_error_train2)\n",
    "compensated_y_test=compensated1_test-(error_predicted_tes2+subtraction_error_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56bab6",
   "metadata": {},
   "source": [
    "# Final NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbd32ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "error_predicted_tr3=error_predicted_tr2+subtraction_error_train2\n",
    "error_predicted_tes3=error_predicted_tes2+subtraction_error_test2\n",
    "\n",
    "training_final_add=np.column_stack((predicted_train, error_predicted_tr))\n",
    "training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "\n",
    "test_final_add=np.column_stack((predicted_test, error_predicted_tes))\n",
    "test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "\n",
    "####final NN\n",
    "m_final=len(training_final_add[0,:])\n",
    "p_final=np.size(y_train[0])\n",
    "N_final=len(training_final_add)\n",
    "\n",
    "final_model= Sequential ([\n",
    "    Dense(hidden4, input_dim=m_final, activation='relu'), \n",
    "#    Dropout(0.1),\n",
    "#    Dense(second_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "#    Dense(third_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "#    Dense(forth_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "    Dense(p_final)\n",
    "])\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "sgd=keras.optimizers.SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "final_model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "final_history=final_model.fit(training_final_add, y_train, batch_size=N_final, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    \n",
    "final_predicted_tr =final_model.predict(training_final_add)\n",
    "final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "final_predicted_tes = final_model.predict(test_final_add)\n",
    "final_predicted_tes = np.reshape(final_predicted_tes, (final_predicted_tes.size,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc80eea",
   "metadata": {},
   "source": [
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f18f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train+subtraction_successive_train\n",
    "final_y_train=final_predicted_tr+subtraction_successive_train\n",
    "final_y_train = np.reshape(final_y_train, (final_y_train.size,))\n",
    "\n",
    "final_error_train=final_y_train-y_train\n",
    "final_rmse_error_train=np.sqrt(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mse_train=(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mape_train=100*sum(abs(final_error_train/y_train))/len(y_train)\n",
    "final_mae_train=sum(abs(final_error_train-y_train))/len(y_train)\n",
    "final_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - final_y_train) / y_train))))\n",
    "\n",
    " \n",
    "y_test=y_test+subtraction_successive_test\n",
    " \n",
    "final_y_test=final_predicted_tes+subtraction_successive_test\n",
    "y_test = np.reshape(y_test, (y_test.size,))\n",
    "final_y_test = np.reshape(final_y_test, (final_y_test.size,))\n",
    "\n",
    "\n",
    "#final_error_test=y_test[:-1]-final_predicted_tes[:-1]\n",
    "final_error_test=final_y_test[:-1]-y_test[:-1] \n",
    "final_rmse_error_test=np.sqrt(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mse_test=(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mape_test=100*sum(abs(final_error_test/y_test[:-1]))/len(y_test-1)\n",
    "final_mae_test=sum(abs(final_error_test-y_test[:-1]))/len(y_test-1)\n",
    "final_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - final_y_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "#errors of the first nn\n",
    "predicted_train=predicted_train+subtraction_successive_train\n",
    "predicted_test=predicted_test+subtraction_successive_test\n",
    "\n",
    "predicted_error_train=predicted_train-y_train\n",
    "predicted_rmse_error_train=np.sqrt(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mse_train=(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mape_train=100*sum(abs(predicted_error_train/y_train))/len(y_train)\n",
    "predicted_mae_train=sum(abs(predicted_error_train-y_train))/len(y_train)\n",
    "predicted_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - predicted_train) / y_train))))\n",
    "\n",
    "predicted_error_test=predicted_test[:-1]-y_test[:-1]\n",
    "predicted_rmse_error_test=np.sqrt(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mse_test=(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mape_test=100*sum(abs(predicted_error_test/y_test[:-1]))/len(y_test-1)\n",
    "predicted_mae_test=sum(abs(predicted_error_test-y_test[:-1]))/len(y_test-1)\n",
    "predicted_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - predicted_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "#errors of the second nn\n",
    "compensated1_train_error=compensated1_train-y_train\n",
    "\n",
    "compensated1_train_rmse_error_train=np.sqrt(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mse_train=(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mape_train=100*sum(abs(compensated1_train_error/y_train))/len(y_train)\n",
    "compensated1_train_mae_train=sum(abs(compensated1_train_error-y_train))/len(y_train)\n",
    "compensated1_train_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated1_train) / y_train))))*100\n",
    "\n",
    "compensated1_test_error=compensated1_test[:-1]-y_test[:-1]\n",
    "\n",
    "compensated1_test_rmse_error_test=np.sqrt(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mse_test=(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mape_test=100*sum(abs(compensated1_test_error/y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_mae_test=sum(abs(compensated1_test_error-y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated1_test[:-1]) / y_test[:-1]))))*100\n",
    "\n",
    "#errors of the third nn\n",
    "compensated_error_train=compensated_y_train-y_train\n",
    "\n",
    "comp_rmse_error_train=np.sqrt(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mse_train=(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mape_train=100*sum(abs(compensated_error_train/y_train))/len(y_train)\n",
    "comp_mae_train=sum(abs(compensated_error_train-y_train))/len(y_train)\n",
    "comp_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated_y_train) / y_train))))*100\n",
    "\n",
    "compensated_error_test=compensated_y_test[:-1]-y_test[:-1]\n",
    "\n",
    "comp_rmse_error_test=np.sqrt(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mse_test=(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mape_test=100*sum(abs(compensated_error_test/y_test[:-1]))/len(y_test-1)\n",
    "comp_mae_test=sum(abs(compensated_error_test-y_test[:-1]))/len(y_test-1)\n",
    "comp_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated_y_test[:-1]) / y_test[:-1]))))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a335a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz_rmse_errors_ttrain=(predicted_rmse_error_train,compensated1_train_rmse_error_train, comp_rmse_error_train,final_rmse_error_train)\n",
    "zz_rmse_errors_test=(predicted_rmse_error_test,compensated1_test_rmse_error_test, comp_rmse_error_test,final_rmse_error_test)\n",
    "\n",
    "zz_rmspe_errors_ttrain=(predicted_rmspe_train,compensated1_train_rmspe_train, comp_rmspe_train,final_rmspe_train)\n",
    "zz_rmspe_errors_test=(predicted_rmspe_test,compensated1_test_rmspe_test, comp_rmspe_test,final_rmspe_test)\n",
    "\n",
    "zz_mape_errors_ttrain=(predicted_mape_train,compensated1_train_mape_train, comp_mape_train,final_mape_train)\n",
    "zz_mape_errors_test=(predicted_mape_test,compensated1_test_mape_test, comp_mape_test,final_mape_test)\n",
    "\n",
    "zz_mae_errors_ttrain=(predicted_mae_train,compensated1_train_mae_train, comp_mae_train,final_mae_train)\n",
    "zz_mae_errors_test=(predicted_mae_test,compensated1_test_mae_test, comp_mae_test,final_mae_test)\n",
    "\n",
    "zz_predictions_train = (y_train, predicted_train,compensated1_train,  compensated_y_train, final_y_train)\n",
    "zz_predictions_test = (y_test,predicted_test,compensated1_test, compensated_y_test, final_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa0109af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.28571828289032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tomorrows prediction\n",
    "final_y_test[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
