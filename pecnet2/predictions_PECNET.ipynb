{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7240cc0",
   "metadata": {},
   "source": [
    "# calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e51ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:14:13.505446: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-22 16:14:13.618286: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:13.618297: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-22 16:14:13.637522: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-22 16:14:14.095617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:14.095676: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:14.095681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from keras import optimizers\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import pywt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48ef0d",
   "metadata": {},
   "source": [
    "# main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ef018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing averages\n",
    "def yy5(input_data):\n",
    "    A=0\n",
    "    B=0\n",
    "    C=0\n",
    "    D=0\n",
    "    outputavg = []    \n",
    "    for X in input_data:\n",
    "        Y=(X+A+B+C+D)/5\n",
    "        outputavg.append(Y)\n",
    "        D=C\n",
    "        C=B\n",
    "        B=A\n",
    "        A=X\n",
    "\n",
    "    return outputavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2e3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construction of outputs\n",
    "def output(inputdata):\n",
    "    out=[]\n",
    "    for i in range(7, len(inputdata)-1):\n",
    "        out.append(inputdata[i+1])\n",
    "    out = np.append(out, [np.nan])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ed28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#successive values \n",
    "def successive(successive):\n",
    "   \n",
    "    input_data=[]\n",
    "    for i in range(7, len(successive)):\n",
    "       \n",
    "        input_data.append([successive[i-3]]+[successive[i-2]]+[successive[i-1]]+[successive[i]])\n",
    "    return input_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853f164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavelet transform\n",
    "def four_wavelets(training):\n",
    "    input_data=np.array(training)\n",
    "    days = input_data[:,0:4]\n",
    "    \n",
    "    \n",
    "    for row in input_data:\n",
    "            (a, d) = pywt.dwt(days, 'haar')\n",
    "            (a2,d2)=pywt.dwt(a, 'haar') \n",
    "            l3=np.append(a2,d2, axis=1)\n",
    "            l2_3=np.append(l3,d, axis=1)\n",
    "            transformed_df=l2_3\n",
    "    \n",
    "    training=transformed_df\n",
    "    \n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configurations\n",
    "hidden1=32\n",
    "second_layer1=32\n",
    "third_layer1=32\n",
    "forth_layer1=16\n",
    "hidden2=32\n",
    "second_layer2=32\n",
    "third_layer2=32\n",
    "forth_layer2=16\n",
    "hidden3=32\n",
    "second_layer3=32\n",
    "third_layer3=32\n",
    "forth_layer3=16\n",
    "hidden4=32\n",
    "second_layer4=32\n",
    "third_layer4=32\n",
    "forth_layer4=16\n",
    "hidden5=32\n",
    "second_layer5=32\n",
    "third_layer5=32\n",
    "forth_layer5=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72bf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling input files\n",
    "#input_data=pd.read_csv('bistclose.csv')\n",
    "#input_data=pd.read_csv('close_is.csv')\n",
    "#input_data=pd.read_csv('open.csv')\n",
    "#input_data=pd.read_csv('close.csv')\n",
    "#input_data=pd.read_csv('min.csv')\n",
    "#input_data=pd.read_csv('max.csv')\n",
    "input_data=pd.read_csv('close.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588bce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construction of input arrays\n",
    "\n",
    "input_data=np.array(input_data)\n",
    "input_data=input_data.reshape(input_data.shape[0])\n",
    "input_data=list(input_data)\n",
    "input_data=np.array(input_data)\n",
    "\n",
    "\n",
    "average=yy5(input_data)\n",
    "input_data_average=successive(average)\n",
    "input_data_successive=successive(input_data)\n",
    "out=output(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "\n",
    "#fig,ax= plt.subplots()\n",
    "#ax.plot(input_data, label='daily_input')\n",
    "#ax.plot(average, label='average_input')\n",
    "#ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00), shadow=True, ncol=2)\n",
    "#plt.title('Input data')\n",
    "#plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b173e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#division of data set into training and test data set\n",
    "N=len(input_data)\n",
    "division_of_training=0.98\n",
    "input_train=input_data_average[:int(N*division_of_training)]\n",
    "input_test=input_data_average[int(N*division_of_training):int(N*1)]\n",
    "\n",
    "successive_train=input_data_successive[:int(N*division_of_training)]\n",
    "successive_test=input_data_successive[int(N*division_of_training):int(N*1)]\n",
    "      \n",
    "second_input_train=successive_train \n",
    "second_input_test=successive_test \n",
    "\n",
    "output_train= out[:int(N*division_of_training)]\n",
    "output_test=out[int(N*division_of_training):int(N*1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50705868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "\n",
    "inputiavg=np.array(input_train)\n",
    "inputiavgt=np.array(input_test)\n",
    "\n",
    "inputsuc=np.array(second_input_train)\n",
    "inputsuct=np.array(second_input_test)\n",
    "\n",
    "subtraction_average_train=inputiavg\n",
    "subtraction_average_test=inputiavgt\n",
    "\n",
    "subtraction_successive_train=inputsuc\n",
    "subtraction_successive_test=inputsuct\n",
    "\n",
    "subtraction_average_train=subtraction_average_train.sum(axis=1)/4\n",
    "subtraction_average_test=subtraction_average_test.sum(axis=1)/4\n",
    "\n",
    "subtraction_successive_train=subtraction_successive_train.sum(axis=1)/4\n",
    "subtraction_successive_test=subtraction_successive_test.sum(axis=1)/4\n",
    "\n",
    "#normalization of inputs\n",
    "first_input_train=input_train-subtraction_average_train[:, None]\n",
    "first_input_test=input_test-subtraction_average_test[:,None]\n",
    "\n",
    "output_train=output_train-subtraction_successive_train\n",
    "output_test=output_test-subtraction_successive_test\n",
    "\n",
    "second_input_train=second_input_train-subtraction_successive_train[:,None]\n",
    "second_input_test=second_input_test-subtraction_successive_test[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2d5a6",
   "metadata": {},
   "source": [
    "# First NN, primary NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b75fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                128       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:14:17.024302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-22 16:14:17.024511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024562: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024710: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024744: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-09-22 16:14:17.024785: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-22 16:14:17.024977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/oguz/Downloads/ENTER/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 513us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#4inputs WT\n",
    "final_first_w_input_train=four_wavelets(first_input_train)\n",
    "final_first_w_input_test=four_wavelets(first_input_test)\n",
    "\n",
    "X_train=np.array(final_first_w_input_train[:, 1:])\n",
    "y_train=np.array(output_train)\n",
    "\n",
    "X_test=np.array(final_first_w_input_test[:,1:])\n",
    "y_test=np.array(output_test)\n",
    "\n",
    "m_primary=len(X_train[0,:])\n",
    "p_primary=np.size(y_train[0])\n",
    "N_primary=len(X_train)\n",
    "\n",
    "model= Sequential ([\n",
    "    Dense(hidden1, input_dim=m_primary, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer1), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_primary)\n",
    "    ])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "sgd=SGD(lr=0.05,momentum=0.75, decay=0.0, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_absolute_error','mean_squared_logarithmic_error','cosine_similarity','logcosh'])\n",
    "history1=model.fit(X_train, y_train, batch_size=N_primary, epochs=300, shuffle=False, verbose=0)  \n",
    "\n",
    "predicted_train = model.predict(X_train) \n",
    "predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "error_train1=predicted_train-y_train\n",
    "\n",
    "predicted_test = model.predict(X_test) \n",
    "predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "error_test1=predicted_test-y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b39a12",
   "metadata": {},
   "source": [
    "# Second NN, error forecasting network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41e6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 32)                128       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "125/125 [==============================] - 0s 595us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "error_train=pd.DataFrame(error_train1)\n",
    "add_train=four_wavelets(second_input_train) \n",
    "   \n",
    "X_error_train1=np.array(add_train[:, 1:])\n",
    "y_error_train1=np.array(error_train)\n",
    "\n",
    "error_test=pd.DataFrame(error_test1)\n",
    "add_test=four_wavelets(second_input_test) \n",
    "  \n",
    "X_error_test1=np.array(add_test[:, 1:])\n",
    "\n",
    "m_second=len(X_error_train1[0,:])\n",
    "p_second=np.size(y_train[0])\n",
    "N_second=len(X_error_train1)\n",
    "\n",
    "error_model1= Sequential ([\n",
    "    Dense(hidden2, input_dim=m_second, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer2), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_second)\n",
    "])\n",
    "\n",
    "error_model1.summary()\n",
    "\n",
    "sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "error_model1.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "history3=error_model1.fit(X_error_train1, y_error_train1, batch_size=N_second, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "error_predicted_tr = error_model1.predict(X_error_train1)\n",
    "error_predicted_tr = np.reshape(error_predicted_tr, (error_predicted_tr.size,))\n",
    "error_predicted_tes = error_model1.predict(X_error_test1)\n",
    "error_predicted_tes = np.reshape(error_predicted_tes, (error_predicted_tes.size,))\n",
    "\n",
    "compensated1_train=(predicted_train+subtraction_successive_train)-(error_predicted_tr)\n",
    "compensated1_test=(predicted_test+subtraction_successive_test)-(error_predicted_tes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12cff4",
   "metadata": {},
   "source": [
    "# Third NN, error network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "314f609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,785\n",
      "Trainable params: 2,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "125/125 [==============================] - 0s 674us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "error_train2a=compensated1_train-(y_train+subtraction_successive_train)\n",
    "error_test2a=compensated1_test-(y_test+subtraction_successive_test)\n",
    "\n",
    "error_train2=pd.DataFrame(error_train2a)\n",
    "error_train2 [1]= error_train2[0].shift(1)\n",
    "error_train2 [2]=error_train2[1].shift(1)\n",
    "error_train2 [3]=error_train2[2].shift(1)\n",
    "error_train2[4]=error_train2[3].shift(1)\n",
    "error_train2 = error_train2.replace(np.nan, 0)\n",
    "\n",
    "##error normalization\n",
    "subtraction_error_train2=np.array(error_train2)\n",
    "subtraction_error_train2=subtraction_error_train2[:,:-1]\n",
    "subtraction_error_train2=subtraction_error_train2.sum(axis=1)/4\n",
    "\n",
    "error_train2=error_train2-subtraction_error_train2[:, None]\n",
    "\n",
    "\n",
    "error_train2=np.array(error_train2)\n",
    "days_train = error_train2[:,1:5]\n",
    "input3_train=four_wavelets(days_train)\n",
    "output3_train=error_train2[:,0:1]\n",
    "\n",
    "X_error_train2=np.array(input3_train[:, 1:])\n",
    "y_error_train2=np.array(output3_train)\n",
    "\n",
    "error_test2=pd.DataFrame(error_test2a)\n",
    "error_test2 [1]= error_test2[0].shift(1)\n",
    "error_test2 [2]=error_test2[1].shift(1)\n",
    "error_test2 [3]=error_test2[2].shift(1)\n",
    "error_test2[4]=error_test2[3].shift(1)\n",
    "error_test2 = error_test2.replace(np.nan, 0)\n",
    "\n",
    "subtraction_error_test2=np.array(error_test2)\n",
    "subtraction_error_test2=subtraction_error_test2[:,:-1]\n",
    "subtraction_error_test2=subtraction_error_test2.sum(axis=1)/4\n",
    "\n",
    "error_test2=error_test2-subtraction_error_test2[:,None]\n",
    "\n",
    "error_test2=np.array(error_test2)\n",
    "days_test = error_test2[:,1:5]\n",
    "input3_test=four_wavelets(days_test)\n",
    "output3_test=error_test2[:,0:1]\n",
    "\n",
    "X_error_test2=np.array(input3_test[:, 1:])\n",
    "\n",
    "\n",
    "#####3rd NN\n",
    "m_error=len(X_error_train2[0,:])\n",
    "p_error=np.size(y_error_train2[0])\n",
    "N_error=len(X_error_train2)\n",
    "\n",
    "\n",
    " \n",
    "error_model2= Sequential ([\n",
    "    Dense(hidden3, input_dim=m_error, activation='relu'), \n",
    "    Dropout(0.1),\n",
    "    Dense(second_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(third_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(forth_layer3), #,activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(p_error)\n",
    "])\n",
    "\n",
    "error_model2.summary()\n",
    "\n",
    "sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "error_model2.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "history4=error_model2.fit(X_error_train2, y_error_train2, batch_size=N_error, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "\n",
    "error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "error_predicted_tes2 = error_model2.predict( X_error_test2)\n",
    "error_predicted_tes2= np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "compensated_y_train=compensated1_train-(error_predicted_tr2+subtraction_error_train2)\n",
    "compensated_y_test=compensated1_test-(error_predicted_tes2+subtraction_error_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56bab6",
   "metadata": {},
   "source": [
    "# Final NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd32ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "125/125 [==============================] - 0s 557us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "error_predicted_tr3=error_predicted_tr2+subtraction_error_train2\n",
    "error_predicted_tes3=error_predicted_tes2+subtraction_error_test2\n",
    "\n",
    "training_final_add=np.column_stack((predicted_train, error_predicted_tr))\n",
    "training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "\n",
    "test_final_add=np.column_stack((predicted_test, error_predicted_tes))\n",
    "test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "\n",
    "####final NN\n",
    "m_final=len(training_final_add[0,:])\n",
    "p_final=np.size(y_train[0])\n",
    "N_final=len(training_final_add)\n",
    "\n",
    "final_model= Sequential ([\n",
    "    Dense(hidden4, input_dim=m_final, activation='relu'), \n",
    "#    Dropout(0.1),\n",
    "#    Dense(second_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "#    Dense(third_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "#    Dense(forth_layer4), #,activation='relu'),\n",
    "#    Dropout(0.1),\n",
    "    Dense(p_final)\n",
    "])\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "sgd=SGD(lr=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "final_model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse','mae','accuracy'])\n",
    "final_history=final_model.fit(training_final_add, y_train, batch_size=N_final, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    \n",
    "final_predicted_tr =final_model.predict(training_final_add)\n",
    "final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "final_predicted_tes = final_model.predict(test_final_add)\n",
    "final_predicted_tes = np.reshape(final_predicted_tes, (final_predicted_tes.size,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc80eea",
   "metadata": {},
   "source": [
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f18f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON =  1e-10\n",
    "\n",
    "y_train=y_train+subtraction_successive_train\n",
    "final_y_train=final_predicted_tr+subtraction_successive_train\n",
    "final_y_train = np.reshape(final_y_train, (final_y_train.size,))\n",
    "\n",
    "final_error_train=final_y_train-y_train\n",
    "final_rmse_error_train=np.sqrt(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mse_train=(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mape_train=100*sum(abs(final_error_train/y_train))/len(y_train)\n",
    "final_mae_train=sum(abs(final_error_train-y_train))/len(y_train)\n",
    "final_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - final_y_train) / (y_train+ EPSILON)))))\n",
    "\n",
    " \n",
    "y_test=y_test+subtraction_successive_test\n",
    " \n",
    "final_y_test=final_predicted_tes+subtraction_successive_test\n",
    "y_test = np.reshape(y_test, (y_test.size,))\n",
    "final_y_test = np.reshape(final_y_test, (final_y_test.size,))\n",
    "\n",
    "\n",
    "#final_error_test=y_test[:-1]-final_predicted_tes[:-1]\n",
    "final_error_test=final_y_test[:-1]-y_test[:-1] \n",
    "final_rmse_error_test=np.sqrt(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mse_test=(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mape_test=100*sum(abs(final_error_test/y_test[:-1]))/len(y_test-1)\n",
    "final_mae_test=sum(abs(final_error_test-y_test[:-1]))/len(y_test-1)\n",
    "final_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - final_y_test[:-1]) / (y_test[:-1]+ EPSILON)))))\n",
    "\n",
    "#errors of the first nn\n",
    "predicted_train=predicted_train+subtraction_successive_train\n",
    "predicted_test=predicted_test+subtraction_successive_test\n",
    "\n",
    "predicted_error_train=predicted_train-y_train\n",
    "predicted_rmse_error_train=np.sqrt(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mse_train=(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mape_train=100*sum(abs(predicted_error_train/y_train))/len(y_train)\n",
    "predicted_mae_train=sum(abs(predicted_error_train-y_train))/len(y_train)\n",
    "predicted_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - predicted_train) /(y_train+ EPSILON)))))\n",
    "\n",
    "predicted_error_test=predicted_test[:-1]-y_test[:-1]\n",
    "predicted_rmse_error_test=np.sqrt(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mse_test=(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mape_test=100*sum(abs(predicted_error_test/y_test[:-1]))/len(y_test-1)\n",
    "predicted_mae_test=sum(abs(predicted_error_test-y_test[:-1]))/len(y_test-1)\n",
    "predicted_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - predicted_test[:-1]) / (y_test[:-1]+ EPSILON)))))\n",
    "\n",
    "#errors of the second nn\n",
    "compensated1_train_error=compensated1_train-y_train\n",
    "\n",
    "compensated1_train_rmse_error_train=np.sqrt(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mse_train=(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mape_train=100*sum(abs(compensated1_train_error/y_train))/len(y_train)\n",
    "compensated1_train_mae_train=sum(abs(compensated1_train_error-y_train))/len(y_train)\n",
    "compensated1_train_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated1_train) /(y_train+ EPSILON)))))*100\n",
    "\n",
    "compensated1_test_error=compensated1_test[:-1]-y_test[:-1]\n",
    "\n",
    "compensated1_test_rmse_error_test=np.sqrt(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mse_test=(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mape_test=100*sum(abs(compensated1_test_error/y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_mae_test=sum(abs(compensated1_test_error-y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated1_test[:-1]) / (y_test[:-1]+ EPSILON)))))*100\n",
    "\n",
    "#errors of the third nn\n",
    "compensated_error_train=compensated_y_train-y_train\n",
    "\n",
    "comp_rmse_error_train=np.sqrt(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mse_train=(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mape_train=100*sum(abs(compensated_error_train/y_train))/len(y_train)\n",
    "comp_mae_train=sum(abs(compensated_error_train-y_train))/len(y_train)\n",
    "comp_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated_y_train) / (y_train+ EPSILON)))))*100\n",
    "\n",
    "compensated_error_test=compensated_y_test[:-1]-y_test[:-1]\n",
    "\n",
    "comp_rmse_error_test=np.sqrt(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mse_test=(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mape_test=100*sum(abs(compensated_error_test/y_test[:-1]))/len(y_test-1)\n",
    "comp_mae_test=sum(abs(compensated_error_test-y_test[:-1]))/len(y_test-1)\n",
    "comp_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated_y_test[:-1]) / (y_test[:-1]+ EPSILON)))))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a335a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz_rmse_errors_ttrain=(predicted_rmse_error_train,compensated1_train_rmse_error_train, comp_rmse_error_train,final_rmse_error_train)\n",
    "zz_rmse_errors_test=(predicted_rmse_error_test,compensated1_test_rmse_error_test, comp_rmse_error_test,final_rmse_error_test)\n",
    "\n",
    "zz_rmspe_errors_ttrain=(predicted_rmspe_train,compensated1_train_rmspe_train, comp_rmspe_train,final_rmspe_train)\n",
    "zz_rmspe_errors_test=(predicted_rmspe_test,compensated1_test_rmspe_test, comp_rmspe_test,final_rmspe_test)\n",
    "\n",
    "zz_mape_errors_ttrain=(predicted_mape_train,compensated1_train_mape_train, comp_mape_train,final_mape_train)\n",
    "zz_mape_errors_test=(predicted_mape_test,compensated1_test_mape_test, comp_mape_test,final_mape_test)\n",
    "\n",
    "zz_mae_errors_ttrain=(predicted_mae_train,compensated1_train_mae_train, comp_mae_train,final_mae_train)\n",
    "zz_mae_errors_test=(predicted_mae_test,compensated1_test_mae_test, comp_mae_test,final_mae_test)\n",
    "\n",
    "zz_predictions_train = (y_train, predicted_train,compensated1_train,  compensated_y_train, final_y_train)\n",
    "zz_predictions_test = (y_test,predicted_test,compensated1_test, compensated_y_test, final_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b87b3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7044040223020223, 1.5643929504662653, 1.2737551694708087, 0.956008658321112)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz_rmspe_errors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfd3487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19226439131066114,\n",
       " 0.17576729715049122,\n",
       " 0.1508291741374295,\n",
       " 0.10729366951105385)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz_rmse_errors_ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa0109af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.27128756, 25.58797245, 26.01526307, 26.10559962])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tomorrows prediction\n",
    "final_y_test[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
